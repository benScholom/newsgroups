{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Notebook for the classification of newsgroups into different categories\"\"\"\n",
    "\n",
    "\n",
    "#import relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "#fetching the datasetm though removing metadata that is useless for text analyis\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "x = dataset.data  # this is the data\n",
    "y = dataset.target  # these are the class labels\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(x)\n",
    "df_text.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic = pd.DataFrame(y,columns=['class_number'])\n",
    "df_topic['class_number'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x dataset has been converted to the data frame df_text - value or sample is a single post from a single user.\n",
    "\n",
    "The y (df_topic) dataset contains the topic label for each post in the x dataset. The labels are represented by numbers. Also note that the data seems relatively balanced, which makes classification less complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "There are many potential steps that can be taken to clean text data. In this notebook, I will do the following:\n",
    "\n",
    "1. Remove unnecesary characters and text sections\n",
    "    - characters such as '\\n', '@', '/', '\\', '<', '>', '#', all digits, etc. do not have any bearing on the subject and can create noise when identifying words\n",
    "    - certain sections, like usernames, email addresses, and subjects lines also are not completely relevant to the subject - most of these were removed in the cell above in which I retrieved the data\n",
    "    - remove punctuation like '.', ';', ':' and ',' as these create noise in identifying words. Apostrophes (') are included in written words in english so they are worth keeping for now; however, for the future it may be worth examining whether it helps to split contractions in component words\n",
    "\n",
    "2. Tokenize text into separate words\n",
    "    - this involves removing stop words including but not limited to: 'the', 'and', pronouns like 'it', 'he' and 'she' and prepositions. These words add no information about the topic of the document\n",
    "    - it is also important to remove capital letters to reduce noise\n",
    "\n",
    "3. Leverage word lemmazation or stemming to remove prefixes, suffixes and generally reduce words to their deictionary form\n",
    "    - this will also involve part of speech tagging for lemmazation, stemming however is simpler and faster but less accurate. I will use stemming for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using nltk library for quick text preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import wordpunct_tokenize as wt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# packages below are used for lemmazation\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk import WordNetLemmatizer as lem\n",
    "# from nltk import pos_tag as pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "FORMAT_WORDS = ['From', 'Subject', 'Lines', 'Organization', 'Date', 'writes']\n",
    "PRONOUNS = ['i', 'he', 'she', 'it', 'they', 'you']\n",
    "STOPWORDS = set(sw.words('english'))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# create function to process each document\n",
    "def clean(text, format_words, stop_words, pronouns, bank, doc):\n",
    "    \"\"\"function that performs all preprocing tasks\"\"\"\n",
    "    #split into lines to remove newline characters\n",
    "    lines = [line for line in text.split('\\n')]\n",
    "    #remove unecesary formating lines (with email addresses and other non-relevant info) and characters\n",
    "    for line in lines:\n",
    "#         check_counter = 0\n",
    "#         for word in format_words:\n",
    "#             if word not in line:\n",
    "#                 check_counter += 1\n",
    "#             elif word in line:\n",
    "#                 continue\n",
    "#         if check_counter == len(format_words):\n",
    "            #remove punction characters and pronouns, also remove random letters than could be middle initials\n",
    "        for word in wt(line):\n",
    "            word = word.strip('_').lower()\n",
    "            if word in stop_words or word in pronouns or len(word) == 1:\n",
    "                continue\n",
    "            if all(char in set(string.punctuation) for char in word):\n",
    "                continue\n",
    "            if any(char in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] for char in word):\n",
    "                continue\n",
    "            #apply stemmer\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            new_word = stemmer.stem(word)\n",
    "            if new_word not in bank.keys():\n",
    "                bank[new_word] = 1  \n",
    "            else:\n",
    "                bank[new_word] += 1\n",
    "            doc.append(new_word)\n",
    "                \n",
    "def create_dicts(dataset):\n",
    "        # bank of all words\n",
    "        wordbank = {}\n",
    "        # dictionary where document indexes are stored for\n",
    "        doc_dict = {}\n",
    "        \"\"\"function to build the dictionary\"\"\"\n",
    "        for  i in range(len(dataset)):\n",
    "            doc_dict[i] = []\n",
    "            clean(dataset[i], FORMAT_WORDS, STOPWORDS, PRONOUNS, wordbank, doc_dict[i])\n",
    "            doc_dict[i] = \" \".join(doc_dict[i])\n",
    "        return wordbank, doc_dict\n",
    "\n",
    "# below function used for lemmazation\n",
    "#def convert_pos(tag):\n",
    "#     \"\"\"converts parts of speech from penn tree format to wordnet format\"\"\"\n",
    "#     poskey = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}\n",
    "#     print(poskey.get(tag[0]))\n",
    "#     return poskey.get(tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the dictionary of word counts among all documents\n",
    "wordbank, doc_dict = create_dicts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wordbank.keys()))\n",
    "sorted(wordbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dictionary by values\n",
    "sort_bank = sorted(wordbank.items(), key=lambda x: x[1], reverse=True)\n",
    "#top 1000 words and counts\n",
    "sort_bank[0: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create word document matrix\n",
    "doc_series = pd.Series(doc_dict)\n",
    "vect = CountVectorizer()\n",
    "matrix = vect.fit_transform(doc_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view sparse matrix\n",
    "df_matrix = pd.SparseDataFrame(matrix, columns=vect.get_feature_names()).fillna(0)\n",
    "# for i, col in enumerate(vect.get_feature_names()):\n",
    "#     df_matrix[col] = pd.SparseSeries(matrix[:, i].toarray().ravel(), fill_value=0)\n",
    "df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using dimensionality reduction with LSA (svd)\n",
    "# this requires the use of a tfidf vectorizer rather than the count vectorizer, \n",
    "# which accounts for cosine similarity and normalizes the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement algorithm, use dimensionality reduction to reduce to 10s or 100s of dimensions\n",
    "CLUSTERS = np.unique(y).shape[0]\n",
    "COMPONENTS = 80\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(doc_series)\n",
    "#output of svd needs to be normalized to improve k-means performance\n",
    "svd = TruncatedSVD(n_components=COMPONENTS)\n",
    "normal = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normal)\n",
    "matrix_reduced = lsa.fit_transform(tfidf_matrix)\n",
    "matrix_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=CLUSTERS, random_state=42)\n",
    "cluster.fit(matrix_reduced)\n",
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance\n",
    "#we should avoid using accuracy as the labels may not correspond exactly, instead use the following measures:\n",
    "#homogeniety examines whether all members of the same cluster have the same class\n",
    "#completeness measures whether all mebers of the same class are in the same cluster\n",
    "#v-measure is an entropy-based measure designed to evaluate whether both homogeniety and completeness have been achieved\n",
    "#these measures should evaluate whether the clustering fit the data well\n",
    "from sklearn import metrics\n",
    "\n",
    "homogeniety, completeness, v_measure = metrics.homogeneity_completeness_v_measure(y, cluster.labels_)\n",
    "print('homogeneity', homogeniety)\n",
    "print('completness', completeness)\n",
    "print('v_measure', v_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "1. To generate features, this example uses TF-IDF, which creates a matrix of documents and word tokens and assigns weights based on both the term frequency, which is used to determine how common a word is in a specific documen, and the inverse document frequency, which is used to determine how common a word is amongst all documents. The use of both these elements in detemining the value of the each feature removes noise from commonly used words and increased word counts in longer documents. This makes the data more relevant to the topic. It is also import to note that there is no dimensionality reduction done here into order to keep as much information as possible.\n",
    "\n",
    "2.  Given that this is a decent size dataset with a lot of features, the Stochastic Gradient Descent Calssifier was chosen due to its ability to deal with lots of data quickly. Hyperparameter optimzation was done with a package to make it go more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "%matplotlib inline\n",
    "\n",
    "TEST = 500/matrix_reduced.shape[0]\n",
    "SPLITS = 10\n",
    "\n",
    "\n",
    "tfidf2 = TfidfVectorizer(stop_words='english')\n",
    "#initialize estimator\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "#separate test set\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(tfidf2.fit_transform(doc_series), y, test_size=TEST, random_state=42)\n",
    "\n",
    "sgd.fit(X_tv, y_tv)\n",
    "y_hat = sgd.predict(X_test)\n",
    "print('Test F1:', metrics.f1_score(y_test, y_hat, average='macro'))\n",
    "print('Test Accuracy:', metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hp\n",
    "#specify boundaries for hyperparater optimization\n",
    "L1L = 0.0\n",
    "L1U = 1.0\n",
    "ALPHAL =  -9 * np.log(10)\n",
    "ALPHAU =  -4 * np.log(10)\n",
    "NITER = 20 + 5 * hp.hp.randint('clf__n_iter', 12)\n",
    "EVALUATION = 10\n",
    "\n",
    "hyp_dict = {}\n",
    "hyp_dict['l1_ratio'] = hp.hp.uniform('clf__l1_ratio', L1L, L1U)      \n",
    "hyp_dict['alpha'] = hp.hp.loguniform('clf__alpha', ALPHAL, ALPHAU)     \n",
    "hyp_dict['n_iter'] = NITER\n",
    "\n",
    "#create variable for use with multiclass analysis\n",
    "score_var = ['f1_macro']\n",
    "#use pipeline for easier hyper parameter optimization\n",
    "\n",
    "def hyper(parameters):\n",
    "    sgd.set_params(**parameters)\n",
    "    kfold = KFold(n_splits=SPLITS, shuffle=True)\n",
    "    score = cross_validate(sgd, X_tv, y_tv, scoring=score_var,\n",
    "                         cv=kfold)\n",
    "    return 1-score['test_f1_macro'].mean()\n",
    "\n",
    "opt = hp.fmin(hyper, hyp_dict, algo=hp.tpe.suggest, max_evals=EVALUATION)\n",
    "\n",
    "params = hp.space_eval(hyp_dict, opt)\n",
    "\n",
    "sgd.set_params(**params).fit(X_tv, y_tv)\n",
    "y_hat = sgd.predict(X_test)\n",
    "print('Test F1:', metrics.f1_score(y_test, y_hat, average='macro'))\n",
    "print('Test Accuracy:', metrics.accuracy_score(y_test, y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
